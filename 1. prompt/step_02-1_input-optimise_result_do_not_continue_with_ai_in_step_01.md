### 0. 上下文文件设置

#### 0.1 附带的二方包等无法引用内容

#### 0.2 文件引用

Tic_Tac_Toe-AI_Reinforcement_Learning_Training--Developed_by_DeepSeek-R1/
├── 1. prompt/
│   ├── file-path-output.py
│   ├── step_01-1_input-init_generate.md
│   └── step_01-2_output-init_generate.md
├── ai/
│   ├── __pycache__/
│   │   ├── __init__.cpython-311.pyc
│   │   ├── agent.cpython-311.pyc
│   │   └── dqn.cpython-311.pyc
│   ├── __init__.py
│   ├── agent.py
│   └── dqn.py
├── game/
│   ├── __pycache__/
│   │   ├── __init__.cpython-311.pyc
│   │   ├── config.cpython-311.pyc
│   │   └── core.cpython-311.pyc
│   ├── __init__.py
│   ├── config.py
│   └── core.py
├── gui/
│   ├── __pycache__/
│   │   ├── __init__.cpython-311.pyc
│   │   └── interface.cpython-311.pyc
│   ├── __init__.py
│   └── interface.py
├── utils/
│   ├── __init__.py
│   └── logger.py
├── LICENSE
├── README.md
├── main.py
├── requirements.txt
└── toy.py

### 1. 业务说明

- 带有界面的井字棋，好看的界面，动感的交互方式
- 带有强化学习

#### 1.1 核心业务说明

1. 需要使用强化学习，让 AI 自我训练 1000 局
2. 井字棋核心就是 3\*3，我希望能比较轻松地改成 4\*4 或者 n\*n 的一个形式，n超过5时，使用五子棋规则
3. 让AI强化学习训练1000局后，与我进行对练
4. 我要看到AI训练的参数
5. 应该需要明显区分训练时候AI下棋进行学习，和与人类下棋需要使用已有训练来下棋。需要知道，与人类下棋是检验成果的时候，不是用来训练的时候
6. 在AI训练的时候，应该设定自己先手或者后手，然后根据先手或者后手来增加削减的分数
7. 这里明显的一个错误就是，在AI训练的时候，没有失败的情况，因为此时AI是双方
8. 所以应该增加一个AI先手或者后手，然后根据先后手，增加成功失败。也就是在AI训练中，虽然下棋都是使用AI已有的训练结果，但是要根据先后手确定是胜利还是失败

#### 1.2 业务约束

1. 可调节参数n用于设定棋盘大小
2. 日志打印完善
3. 尽量使用GPU进行训练
3. 尽量使用GPU进行训练
3. 尽量使用GPU进行训练
3. 尽量使用GPU进行训练
4. 需要保证训练若干盘之后有明显提升
5. 需要可以保存\读取训练的结果
6. 二次训练时，需要使用之前的训练结果
7. 保存模型数据时，应标识体现棋盘大小n，否则会因为修改数字n而出现错误

#### 1.3 核心逻辑说明

### 2. 可复用资源

#### 2.1 复用方法

#### 2.2 复用数据结构

#### 开发约束

- 高质量代码
- 高可读性代码
- 高质量类注释、高质量方法注释、高质量行注释
- 大量注释
- 慎用 log.error 打印日志
- 组装逻辑里，要尽可能一次性查询，然后再进行组装
- 能复用方法的方法尽量复用
- 应优化文件结构，使用最佳文件结构打印并输出，方便我后续做文件修改
- 严谨使用略等字眼，要把需要的部分全部打出来
- 包含 import 信息

#### 开发预期

- 符合业务要求
- 尽可能打印关键日志
- 尽可能抽出可复用方法

#### 使用技术

- Python
